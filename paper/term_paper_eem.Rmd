---
title: 'Stay home and let the simulation play'
author: 'Nils Paffen, David Schulze'
subtitle: "Predicting regional football league outcomes with statistical methods "
type: "Working Paper"
discipline: ""
date: "today"
studid: "3071594 (Nils Paffen)"
supervisor: "Prof. Dr. Christoph Hanck"
secondsupervisor: "NA"
estdegree_emester: "Summer Term 2020"
header-includes:
   - \usepackage{dcolumn, xcolor, amsmath}
output:
  pdf_document:
    keep_tex: yes
    template: template.tex
    fig_caption: yes
    citation_package: biblatex
    number_sections: true
toc: true
lot: true
lof: true
graphics: true
biblio-title: References
fontsize: 12pt
geometry: lmargin = 3 cm,rmargin = 2.5cm,tmargin=2.5cm,bmargin=2.5cm
biblio-files: references.bib
classoption: a4paper
---


```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
library(knitr)
library(tidyverse)
require(stargazer)
require(kableExtra)
require(gridExtra)
source(here::here("src/functions.R"))
## load data or something for preparation

```



# Abstract

Publicly available data and public attention are contributing to the interest in forecasting football game results and the relevance of the accuracy of those forecasts. Global pandemics like SARS-CoV2 are just one reason why seasons may be canceled, providing a regular reason to forecast missing games. We provide a short overview on the state of the literature and use data from the aborted German local men's league season 2019-20 to predict the season’s outcome using three different statistical approaches. A measure of each team’s strength is calculated from past games and used as quantifier in the simulation or prediction. Instead of annulling the games played thus far or using the table as of now, using a prediction algorithm to simulate that seasons end result might be fairer. That's because the algorithm includes the played games to make a better guess at the outcome of the missing games. Research has shown that measures like the Elo rating system are better predictors of a team's performance than for example current league table points on their own. For this data set we find that gains from using advanced methods are marginal when evaluating them with data from past seasons. Methods are evaluated by calculating the correlation of the forecast ranking results for previous seasons with their actual outcomes.


# Introduction

The Covid-19 epidemic forced sports leagues in Germany to suspend championships that were already in full swing. For example, the local men's league Recklinghausen class A1 finished around 150 games, before all further matches were canceled starting from Sunday March 12, 2020, leaving around 90 games left unplayed until the last planned day of the league on Sunday May 24, 2020. It was very likely at the time that the games could not be postponed to a later date, which turned out to be the case. So it was natural for players and fans alike to ask the question: "What would the outcome of the season have been?" We use data on games already played from the website [fussball.de](http://www.fussball.de/spieltagsuebersicht/re-kl-a-1-kreis-recklinghausen-kreisliga-a-herren-saison1920-westfalen/-/staffel/027II28DH8000009VS5489B3VS3GHJJU-G#!/) to answer this question, drawing on established forecasting methods from the literature.

The league system in Germany is structured so that everyone plays two games against every opponent: The system colloquially known as 'Back and Forth' implies that each pair plays once at home and once away in the other team's stadium. This means it’s easier to forecast than the tournament system of the World Cup. There in the group stage, groups are determined by chance, a process known as “seeding”. Groups then play a so-called round-robin tournament, also known as all-play-all, were all group members play against each other, which corresponds to the mode in which the German local leagues play in each round. But the World Cup then continues with single-elimination, or a knock-out stage, which introduces random path dependencies that are not relevant for forecasting the Germany's local leagues. This implies that the part of the existing literature on forecasting results in the FIFA World Cup concerning the group stage remains highly relevant for the task at hand, since the game rules are otherwise identical.

In the next section, we give an overview of models used and evaluated for the purpose of predicting football match outcomes in the past. We introduce a small subset of models in more detail in the third part. The fourth part contains the results from calculating a simulation based on these for the local men's league Recklinghausen class A1. We also present some comparative statistics of the model performance and draw some conclusions in the last segment.


# Literature

A natural starting point for the forecasting of match or season outcomes in football tournaments is using the FIFA points ranking method that is widely used to evaluate the strength of a team and updated after each game. For example, a recent study by Correa et al. -@correa uses FIFA points to forecast the results of the 2018 FIFA Men's World Cup. This approach has however generated criticism @mchale2007, especially because it does not update based on new information fast enough. The benchmark study by Lasek et al. -@lasek2013 compares established and proposed rankings. They find that FIFA rankings perform slightly worse than alternative methods, especially a version of the Elo rating system originally proposed by Arpad Elo for the United States Chess Federation to rate competitive chess players that was adapted for football championships by the authors of the website EloRatings.net -@eloratings. 

Other studies show the effective prediction power of FIFA rankings, e.g. @suzuki2008. Leitner et al. -@leitner2010 find that bookmakers odds are more predictive than FIFA rankings. In our case we don’t expect betting markets to be active enough to make this a feasible approach, although it would be an interesting reference point. We do however adopt their use of Spearman’s rank correlation between simulated and real final tournament rankings to evaluate models’ performance and complement it with Kendall’s tau. Lasek et al. -@lasek2013 evaluate using rating points, which are less relevant for our use case than the absolute rankings, which determine whether a team advances, stays or drops out of a league.

We consider three models for our calculation: First, a benchmark model based on the table points of each team at the time when the league was aborted. Second, an Elo rating system, and third a simple model based on the Poisson distribution.

The benchmark model calculates the probability of winning a match by dividing a team's current points (victories are 3, draws are 2) by the total of their and their opponent's points, we can call this the "points model". This model does not include the possibility of a draw. The probability is not updated after each simulated game, because this does not generate new information about a team’s strength. Averaging the results over enough simulations, this approach will converge to the current table ranking, so it is in fact just a weighted randomization of the current table.

The second model is based on a version of the Elo rating system published anonymously on the website EloRatings.net -@eloratings. The algorithm was originally developed for ranking chess players. As an “earned” rating system (@lasek2013) a team’s rating is updated iteratively according to the outcome of single matches and depending on the expected outcome with regard to the opponent’s rating. This version was especially adapted for the use in ranking football teams. Glickman -@glickman1995 offers a comprehensive discussion of the Elo rating system. Surprisingly, there is no obvious way of predicting the probability of a draw. Therefore we use the empirical average probability for each match.

The third model is a regression model that approximates the distribution of goals in each game to a Poisson or negative binomial distribution with an estimated constant parameter to adjust for the home advantage. This approach follows the literature influenced by @maher1982 and others. Generally, these models include different parameters to allow for team-specific strengths when playing home or away, and while defending or attacking. Other parameters that include for example random effects can be added. We omit them here for simplicity. For a general discussion see @karlis2003. Many extensions of this model as well as model selection algorithms are possible.

For a more recent review of advances in the literature and a new approach based on the Weinbull distribution we refer to @boshnakov2017. They use an evaluation based on calibration curves as well as the payoff from betting strategies and find that their model improves on previous models and can yield positive betting returns. 



# Data

For our simulation study we decided to use data from the local men's league Recklinghausen class A1 in Westphalia, Germany, for the 2019/2020 season. 16 clubs were scheduled to play against each other on a total of 30 match days in one home and one away game each. Due to the SARS-CoV2 pandemic, the association has decided to cancel all matches from March 15, 2020. 20 match days have already been played up to this point in time which corresponds to a database of 158 matches. As the first half of the season had already been completed, each team had already played at least once against each team in the league. The extraction of real data from websites using scraping scripts can be complicated, as website operators have an interest in protecting their data from such automated queries. "Fussball.de" is a website of the DFB (German Football Association) which acts as a collection point for match results and news, especially in the amateur sector. In its [terms and conditions page](http://www.fussball.de/terms.and.conditions#!/), the DFB GmbH restricts the permanent storage of content from the website and commercial use. Therefore we can't store and or share the original data, but only the code we used to create them and the results that we derived from them.

The game results cannot be extracted directly from the website. They are masked, so they are made unreadable when viewing the HTML file and are only evaluated afterwards using JavaScript and transferred to the CSS of the site. The site also offers a match report, which graphically represents a temporal course of the match. This is broken down in the HTML code, in contrast to the match results, unmasked, and shows the course of the match in text form. With the help of regular expression operations, the game result can be reconstructed. The data record was then divided into completed and missing games. The latter amount to 89 in this season, which were simulated with the methods described in the following sections.

Table 1 shows the results after the 20th match day of the local men's league Recklinghausen class A1 in Westphalia for the 2019/2020 season. The column _Goal Diff._ describes how many goals a team scored subtracted by the amount of goals scored against them. Both numbers can be extracted from the former column _Goal Relation_, where the first number describes the amount of scored goals of a team. The club 'VfL Ramsdorf' is leading the league with 52 Points and a goal difference of 53. Followed by 'TuS Gahlen' which is 6 points short in the competition for the league's first place. The other end of the table shows the club 'SV Altendorf Ulfkotte' on the last place with only 7 points and a goal difference of -84. It is preceded by 'Adler Weske II' which achieved 10 points and secured position 15 with a goal difference of -41. Especially the two first and the two last positions in most football leagues are of special interest since those teams could be promoted from the present league called 'Kreisliga' to the next higher league called 'Bezirksliga' or relegated to the lower league called 'Kreisliga B'. Given these results, we expected a high chance for the club 'VfL Ramsdorf' to secure a promotion spot while we expected a fight against relegation between 'TuS 05 Sinsen II', 'Adler Weske II' and 'SV Altendorf-Ulfkotte'.

```{r , echo=FALSE, results = "asis"}
season_overview <- readr::read_rds(stringr::str_c(here::here() , "data", "database_season_1920.rds", sep = "/")) 

season_overview %>% filter(matchday == 20) %>% dplyr::select(-season,-matchday) %>% dplyr::select(rank,club_name,games,everything()) %>%
  rename(Rank = rank, Games = games, Club = club_name, Wins = wins, Ties = ties, Loss = loss, "Goal Relation" = goal_relations, "Goal Diff." = goal_diff, Points = points )  %>% 
  kable(caption = 'Results after Matchday 20 Season 19/20', booktabs = T) %>%
  kable_styling(full_width = F, latex_options =c('striped', 'HOLD_position', 'scale_down'), position = 'center' )
```


For further analysis we decided to scrape the data of season 16/17, 17/18 and 18/19, so we could perform out-of-sample error (OOSE) test statistics. The latter will indicate the predictive performance of the different methods. 



# Predictive Models

To predict the outcome of the canceled games, we determine each team's relative strength using each of the three candidate models. Based on this we simulate or predict the end of the 2019/2020 season by calculating a winning probability for each missing game. Specifically we implement

\begin{itemize}
\item the points model,
\item the EloRankings.net model, 
\item a Poisson/negative binomial regression model
\end{itemize}

The first model is just a simple baseline model that calculates the probability of a team _A_ winning a game against team _B_ using the formula

\begin{align}
P(\text{\emph{A wins}}) = \frac{table points_A}{{table points}_A + {table points}_B},
\end{align}

where _tablepoints_ corresponds to the number of games won at the current state of the season (valued at three points) plus the number of draws (valued at one point). This value also governs the ranking and ultimate placement of the teams in the league. Two issues arise from this approach. Firstly, should the probability based on table points be updated after each simulated game? We argue no, because this would not include new information about the relative strength of the teams and just increase the variance of the result.

Secondly, the average over many simulated runs will converge to the initial table from when the season was interrupted. This will defeat the purpose of running a simulation in the first place, because it does not yield any new information, and we could have just used the table as it was. In case a random draw is used to determine the result, this method is however preferable to an unweighted coin toss, because that would unfairly favor below-average teams.

Our second model is based on the rating algorithm from [eloratings.net](eloratings.net/about). The anonymous site operator uses the following formulation of the rating. Here, the new rating $R_n$ is defined as an update of $R_0$, which is the old rating. The rating is updated, if the outcome differs from what the old rating predicted. The size of the update, or the learning speed of the algorithm, is moderated according to the importance of the match and the goal difference.

\begin{align}
R_n = R_0 + K * (W - W_e) &
\end{align}

\begin{align}
K = \begin{cases}
    K_0         &\mbox{if won/lost by 1 goal} \\
    K_0 * 1.5   &\mbox{if won/lost by 2 goals} \\
    K_0 * 0.75  &\mbox{if won/lost by 3 goals} \\
    K_0 * \left( 0.75 + \frac{N-3}{8} \right) &\mbox{if won/lost by 4 or more goals}
  \end{cases}
\end{align}

The weighting factor _K_ for each match is based on $K_0$, the type of tournament in which the match takes place. The values range from World Cup finals (60) to friendly matches (20). Regular tournament matches are given a weighting factor of 30, which we also use here. $K_0$ is then adjusted for the goal difference of the game outcome (how significant a victory or loss was) according to the formula above. A higher goal difference will have a higher impact on the final rating. _W_ is the result of the match: 0 for a loss, 0.5 for a draw and 1 for a win. $W_e$ is the probability of winning defined by the following formula:

\begin{align}
W_e = \frac{1}{10^{-dr/400}+1,}
\end{align}

where _dr_ is defined as the rating difference plus an arbitrary bonus of 100 points for the home team. This bonus is considered to be a psychological advantage resulting from the fact that the game is played on the home pitch (see, e.g., @Pollard2008). 

To simulate the outcome of the league with the points and Elo ranking method, we follow Correa et al. -@correa and draw the results of each game from a Bernoulli distribution with the parameter _p_ equal to the winning probability obtained from each method.

To account for draws when simulating the outcome with the Elo method, we use the share of draws in the current season as the likelihood of a draw. Half of this is deducted from the home team's winning probability and half from the away team's winning probability. If this puts any probability into negative, this amount is then added to the opposite probability. Then the game result is drawn from this distribution. We repeated this procedure for all games and evaluated the results with 3 points for the winning team, 1 point for both teams in case of a draw and 0 points for the losing team. 

Our third model uses regressions based on the Poisson and negative binomial distributions to predict match outcomes as the number of goals scored in each match. The probability matrix from which the game result is drawn is a $n \times n$ matrix where each cell indicates the probability of that specific match result. While the rows indicates the goals of the home team, the column indicates the goals of the away team. For example the cell of the first row and in the first column indicates the likelihood that both teams score $0$ goals. The maximum number of goals _n_ can be set high enough to cover the range of possible outcomes. Because the highest number of goals scored since 2016 in any one game was 14, we set _n_ equal to 15. The Poisson probability mass function of our model can be expressed as:

\begin{align}
P(x) = \dfrac{e^{-\lambda}\lambda^x}{x!}, \lambda > 0
\end{align}

where $\lambda$ represents the average number of goals. First, we estimate a log-linear Poisson regression model using data from the matches already played:

\begin{align}
log(\text{\emph{goals by team}}) \sim constant + \beta_1 home + \sum_{i}^{T} \beta_i team_i + \sum_{j}^{T} \beta_j opponent_j
\end{align}

Here _goals by team_ represents the number of goals scored by the team in a certain game. The dummy variable _home_ equals 1 if the team plays on it's home pitch, and $team_i$ and $opponent_j$ represent dummies for each team out of a total of _T_ teams respectively. So each game is represented twice: Once from the defending home team and once from the opponents view. Because it is a model in log-transformation, the exponential of the parameters $\beta_i$ and $\beta_j$ then represent the change in expected number of goals scored by the home team. The exponential of the constant specifies the overall expected value of goals if a team plays away, and $e^{\beta_1}$ is the expected effect of the home field advantage. This could be seen as an improvement over the Elo model, because we do not fix this effect in advance.

To estimate the Poisson model, we use the _glm_ function from the R _stats_ package. With the option _family = quasipoisson(link=log)_, a quasi-Maximum likelihood estimation of the log-transformation is calculated. The reason for choosing a Quasi-Poisson distribution is that this adjusts the standard-errors for overdispersion (the variance is larger than the mean). The resulting predictions do not change if one instead uses the standard Poisson distribution. A test from the _AER_ package finds low significance for overdispersion
(estimate: 1.13, z = 1.5215, p-value = 0.064). Pearson's Chi-Squared goodness-of-fit test finds that the model does not fit the data very well (p = 0.00) The residual deviance (388.56) using the Poisson distribution is much larger than the degrees of freedom (284), so we estimate the model with a negative binomial distribution. We use the _glm.nb_ function from the _MASS_ package, which estimates the dispersion parameter using alternating score and information iterations.

```{r , echo = FALSE, results = "asis", message = FALSE}
d <- readr::read_rds(stringr::str_c(here::here() , "data", "database_match_results_1920.rds", sep = "/")) 
d <- bind_rows(
  tibble(
    goals = d$goals_team_home,
    team = d$club_name_home,
    opponent=d$club_name_away,
    home= "y"),
  tibble(
    goals=d$goals_team_away,
    team=d$club_name_away,
    opponent=d$club_name_home,
    home= "n")
)

# create a fake model
# note that team needs to include all of your factors
fake <- lm(goals ~ home + team , d)
# rename the coefficients
names(fake$coefficients) <- gsub("team","",names(fake$coefficients))

# goodness-of-fit test function
gfit <- function(model){
  with(model, cbind(res.deviance = deviance, df = df.residual,
  p = pchisq(deviance, df.residual, lower.tail=FALSE)))
}

# 
m <- glm(goals ~ home + team + opponent, family = poisson(link=log), data = d)
m.s <- summary(m)
#gfit(m)

# Test for overdispersion
#library(AER)
#dispersiontest(m)

# overdispersion robust
m <- glm(goals ~ home + team + opponent, family = quasipoisson(link=log), data = d)
m.s <- summary(m)

# alternative: negative binomial distribution
library(MASS)
nb = glm.nb(goals ~ home + team + opponent, link = "log", data = d)
nb.s <- summary(nb)
#gfit(nb)

## write a function that fixes the names in the glm output
f <- function(x){
  names(x) <- gsub("team|opponent","", names(x))
  return(x)
}

stargazer(fake,fake,fake, header = FALSE,
          # coefficients
          coef = list(
            f( nb.s$coefficients[grepl("Intercept|home", rownames(nb.s$coefficients)), 1]),
            f( nb.s$coefficients[grepl("team", rownames(nb.s$coefficients)), 1]),
            f( nb.s$coefficients[grepl("opponent", rownames(nb.s$coefficients)), 1])
          ),
          # standard errors
          se = list(
            f( nb.s$coefficients[grepl("Intercept|home", rownames(nb.s$coefficients)), 2]),
            f( nb.s$coefficients[grepl("team", rownames(nb.s$coefficients)), 2]),
            f( nb.s$coefficients[grepl("opponent", rownames(nb.s$coefficients)), 2])
          ),
          title  = "19/20 Season Regression Output for the Negative Binomial Model"  ,
          column.labels = c("control","team", "opponent"),
          colnames = FALSE,
          # calculate pvalue using supplied coeff and se
          t.auto = T,
          #dep.var.caption  = "Dependend variable = goals",
          #out = "stargazer_data.html",
          #omit.stat=c("all"),
          type = "latex",
          single.row = TRUE,
          order = c("Constant", "home"),
          align = TRUE,
          column.sep.width = "-50pt",
          font.size = "small",
          style = "aer"
          )
#Template table layout?
```

Note here that "1. SC BW Wulfen" is chosen as the base and no dummy for it is included to avoid perfect multicollinearity. Its parameter is set to zero and all other parameters can be seen as the deviation from it. Also, for season 16/17 the team "VfB Hüls II zg." forfeit before the season started. Therefore all games were canceled and the team was assigned to the last place of the seasons table (place 18).

The coefficients of the model show that the club "Altendorf-Ulfkotte", is least likely to score a goal (low team estimate) and teams playing against them have the highest chance to score (high opponent estimate), with both estimates being highly significant. Since the club is in the last place in the current table, as mentioned in the data section, this is the expected result. Conversely, we observe the something like the opposite for the current table leader "VfL Ramsdorf". For the simulation the result is drawn from the negative binomial and Poisson distributions, and the score probabilities are based on the estimated parameters.

The results from the estimation with a Poisson distribution are very similar. However, the residual difference is higher (388.56 compared to 354.50), indicating a worse fit. The goodness-of-fit test also rejects the model (p = 0.00). The estimation results for Poisson can be found in the Appendix. 

Instead of simulating the result as with the first two methods, we can simply calculate the expected number of goals based on the regression estimate and sum up the expected final league table.

We can evaluate the fit of the distributions also by looking at histograms comparing the actual distributions in the seasons before against the model's predictions. For this purpose we pool the values of all away and home goals of the last three seasons and compare actual with predicted values. As can be seen in the bar plot below, the model does not predict enough outliers of high goal numbers and predicts too many one- and two-goal outcomes. It seems likely that using different distributions can improve the fit.

```{r echo = FALSE}
include_graphics(paste0(getwd(), "/paper/plots/nbinom_actual.png"))


```

```{r echo = FALSE}
include_graphics(paste0(getwd(), "/paper/plots/poisson_actual.png"))


```
A few alternatives have been developed for forecasting football games. Approaches with independent Poisson distributions were improved by introducing correlation between the teams playing against one another in a bivariate Poisson distribution (@karlis2003).

Boshnakov et al. -@boshnakov2016 use a Weibull count model to improve on the bivariate Poisson model, allowing them even to outperform betting market in selected bets. Further analysis could compare the predictive performance of models based on other distributions as well.



# Results

Simulation for the Elo model was repeated until the rate of change of the point average was 1% or less. Aggregation to this point occurred after about 2580 runs.

Looking at the final predictions for the 19/20 season, a few observations stand out. First and foremost, the two relegation spots at the top and bottom end of the table stay the same in each simulation. In the middle field however, the models disagree and rate the teams there very close to each other. The points and Elo method predict more similarly, as well as (Quasi-)Poisson and the negative binomial model. The last two are almost identical. Only TuS Velen gains an extra point in (Quasi-)Poisson, which doesn't affect the ranking. Quasi- and standard Poisson results are identical.

```{r echo=FALSE, results = "asis"}
sim_output_elo <- readRDS(paste0(getwd(), "/data/elo_ties_simulation.rds"))

# 2. evaluate result

all_final_tables_elo <- sim_output_elo$all_final_tables %>% rename(score = points)
all_final_tables_elo <- add_run_rank_col(x = all_final_tables_elo)
all_avg_tables_elo <- sim_output_elo$all_avg_tables
all_avg_tables_elo <- add_run_rank_col(x = all_avg_tables_elo)

# average table result
average_table_elo <- all_avg_tables_elo[
  (nrow(all_avg_tables_elo)-15):nrow(all_avg_tables_elo),] 
average_table_elo <- average_table_elo %>% arrange(rank) %>% dplyr::select(rank, everything(), -run) %>% rename(club_e = club_name, score_e = score)
average_table_elo$score_e <- round(average_table_elo$score_e, 2)
```

```{r echo=FALSE}
sim_output_point <- readRDS(paste0(getwd(), "/data/point_simulation.rds"))
# 2. evaluate result
all_final_tables_point <- sim_output_point$all_final_tables
all_final_tables_point <- add_run_rank_col(x = all_final_tables_point)
all_avg_tables_point <- sim_output_point$all_avg_tables
all_avg_tables_point <- add_run_rank_col(x = all_avg_tables_point)

# average table result
average_table_point <- all_avg_tables_point[
  (nrow(all_avg_tables_point)-15):nrow(all_avg_tables_point),] 
average_table_point <- average_table_point %>% arrange(rank) %>% dplyr::select(rank, club_name, score) %>% rename(club_pnt = club_name, score_pnt = score)
average_table_point$score_pnt <- round(average_table_point$score_pnt, 2)

```

```{r echo=FALSE}
average_table_poisson <- readRDS(paste0(getwd(), "/data/poisson_score_simulation_1920_predict.rds")) %>% 
  dplyr::select(rank, club_name, points) %>%
  dplyr::rename(club_psn = club_name, score_psn = points)

average_table_nbiom <- readRDS(paste0(getwd(), "/data/nbiom_score_simulation_1920_predict.rds")) %>% 
  dplyr::select(rank, club_name, points) %>%
  dplyr::rename(club_nbin = club_name, score_nbin = points)

all_results <- inner_join(average_table_point,average_table_elo, by = "rank") %>%
  inner_join(average_table_poisson, by = "rank") %>%
  inner_join(average_table_nbiom, by = "rank")
kable(all_results,
      caption = "Simulated Final Score Table",
      booktabs = T 
) %>%
  add_header_above(c(" " = 1, "Points Model" = 2, "Elo Rating Model" = 2, "(Quasi-)Poisson Model" = 2, "Neg.Binomial Model" = 2 )) %>%
  kable_styling(latex_options = c("striped", "HOLD_position", "scale_down"),full_width = F)

  

```


```{r echo=FALSE}
library(readr)
library(tidyverse)
poisson_plot<- function(sim_years){
  database_mr <- read_rds(here::here(paste0("/data/database_match_results_",sim_years,".rds")))%>% filter(between(matchday, 1, 20))
  
  database_season <- read_rds(here::here(paste0("/data/database_season_",sim_years,".rds")))
  
  missinggames <- read_rds(here::here(paste0("/data/database_match_results_",sim_years,".rds"))) %>% .[1:4]
  
  
  
  source(here::here("src/functions_N.R"))
  #Idee für Untenschieden : Ausrechen wie häufig unentschieden in dieser Saison gespielt wurde. 
  #Annahme : Zwei gleichstarke Mannschaften haben eine höhere Wahrscheinlichkeit unentschieden zu spielen als zwei unterschiedlich Starke.
  #Ableitung : Für gleichstarke Mannschaften Durchschnittswert für unterschiedlich Starke den Wert diskontieren 
  #Proportion zum Rankingunterschied. 
  
  
  
  home_goals_avg <- database_mr$goals_team_home %>% mean()
  
  away_goals_avg <- database_mr$goals_team_away %>% mean()
  
  
  poisson_model <- 
    bind_rows(
      tibble(
        goals = database_mr$goals_team_home,
        team = database_mr$club_name_home,
        opponent=database_mr$club_name_away,
        home=1),
      tibble(
        goals=database_mr$goals_team_away,
        team=database_mr$club_name_away,
        opponent=database_mr$club_name_home,
        home=0)) %>%
    
    glm(goals ~ home + team +opponent, family=poisson(link=log),data=.)
  summary(poisson_model)
  
  
 sim <- f_simulate_score_prob(foot_model = poisson_model,
                                        homeTeam = missinggames$club_name_home,
                                        awayTeam = missinggames$club_name_away
  ) 
poisson <- tibble(all_goals = c(sim$home, sim$away))  
return(poisson)
 }


sim_years <- c(1617,1718,1819)
poisson_method <- map_df(sim_years, ~poisson_plot(.x)) 



actual <- rbind(read_rds(here::here(paste0("/data/database_match_results_",sim_years[1],".rds"))) %>% .[,5:6],
                read_rds(here::here(paste0("/data/database_match_results_",sim_years[2],".rds"))) %>% .[,5:6],
                read_rds(here::here(paste0("/data/database_match_results_",sim_years[3],".rds"))) %>% .[,5:6])

actual <- tibble(all_goals = c(actual$goals_team_home, actual$goals_team_away))

#png(file = here::here(paste0("paper/plots/poisson_actual_home.png")) )


png(file = here::here(paste0("paper/plots/poisson_actual.png")))
poisson <- tibble(poisson = poisson_method$all_goals,
               actual = actual$all_goals
)%>% gather(key=Type, value=Value) %>% 
  ggplot(aes(x=Value,fill=Type)) + 
  geom_histogram(position="dodge")+
  labs(x = 'Goals per Match' )+
  labs(x = 'Goals per Match', y = 'Frequency of Goals' )+ 
  scale_x_continuous(breaks = 0:20)+
  scale_y_continuous(breaks = seq(0,800,50))
print(poisson)
#dev.off()
```


```{r echo=FALSE}
library(readr)
library(tidyverse)
nbinom_plot<- function(sim_years){
  database_mr <- read_rds(here::here(paste0("/data/database_match_results_",sim_years,".rds")))%>% filter(between(matchday, 1, 20))
  
  database_season <- read_rds(here::here(paste0("/data/database_season_",sim_years,".rds")))
  
  missinggames <- read_rds(here::here(paste0("/data/database_match_results_",sim_years,".rds"))) %>% .[1:4]
  
  
  
  source(here::here("src/functions_N.R"))
  #Idee für Untenschieden : Ausrechen wie häufig unentschieden in dieser Saison gespielt wurde. 
  #Annahme : Zwei gleichstarke Mannschaften haben eine höhere Wahrscheinlichkeit unentschieden zu spielen als zwei unterschiedlich Starke.
  #Ableitung : Für gleichstarke Mannschaften Durchschnittswert für unterschiedlich Starke den Wert diskontieren 
  #Proportion zum Rankingunterschied. 
  
  
  
  home_goals_avg <- database_mr$goals_team_home %>% mean()
  
  away_goals_avg <- database_mr$goals_team_away %>% mean()
  
  
  nbinom_model <- 
    bind_rows(
      tibble(
        goals = database_mr$goals_team_home,
        team = database_mr$club_name_home,
        opponent=database_mr$club_name_away,
        home=1),
      tibble(
        goals=database_mr$goals_team_away,
        team=database_mr$club_name_away,
        opponent=database_mr$club_name_home,
        home=0)) %>%
    
    MASS::glm.nb(goals ~ home + team +opponent,data=.)
  summary(nbinom_model)
  
  
  sim <- f_simulate_score_prob(foot_model = nbinom_model,
                               homeTeam = missinggames$club_name_home,
                               awayTeam = missinggames$club_name_away
  ) 
  nbinom <- tibble(all_goals = c(sim$home, sim$away))  
  return(nbinom)
}


sim_years <- c(1617,1718,1819)
nbinom_method <- map_df(sim_years, ~nbinom_plot(.x)) 



actual <- rbind(read_rds(here::here(paste0("/data/database_match_results_",sim_years[1],".rds"))) %>% .[,5:6],
                read_rds(here::here(paste0("/data/database_match_results_",sim_years[2],".rds"))) %>% .[,5:6],
                read_rds(here::here(paste0("/data/database_match_results_",sim_years[3],".rds"))) %>% .[,5:6])

actual <- tibble(all_goals = c(actual$goals_team_home, actual$goals_team_away))

#png(file = here::here(paste0("paper/plots/nbinom_actual_home.png")) )


png(file = here::here(paste0("paper/plots/nbinom_actual.png")))
nbinom <- tibble(nbinom = nbinom_method$all_goals,
                  actual = actual$all_goals
)%>% gather(key=Type, value=Value) %>% 
  ggplot(aes(x=Value,fill=Type)) + 
  geom_histogram(position="dodge")+
  labs(x = 'Goals per Match' )+
  labs(x = 'Goals per Match', y = 'Frequency of Goals' )+ 
  scale_x_continuous(breaks = 0:20)+
  scale_y_continuous(breaks = seq(0,800,50))
print(nbinom)

```

# OOSE Test Statistics

Making predictions of events that might never happen can obviously be criticized with a simple question. How do you know that your results reflect reality as good as possible? Following George E. P. Box who is known for his quote "All models are wrong", which is often amended by "but some are useful", we want to show that our models cover the latter. The out-of-sample error test statistic is one way to achieve this. One simply divides a data set into a small test data set and a larger training data set. For the seasons of 16/17, 17/18 and 18/19, we split the data sets after the number of games, after which the SARS-Cov2 pandemic forced the 19/20 season to abort. This way we increase the relevance for our use case.

Following @leitner2010, we evaluate the models’ performance using the rank correlation between their predicted and the real ranking tables for the three past years’ seasons (2016, 2017 and 2018). We show both Kendall's tau and Spearman's rho rank correlation coefficients. Generally, no model performs best in all seasons. The Elo method however performs generally better than the regression models' predictions. Surprisingly, the benchmark points method performs best in two out of three seasons. 

Because no model is clearly better then the others, and performance varies a lot between seasons, we see no clear evidence that one method should be preferred. A simulation based on these predictions would however be much fairer than a coin toss.

\input{rank_corr.tex}

```{r echo = FALSE}
ranking_results <- readRDS(file = paste0(getwd(), "/paper/plots/ranking_results.rds"))
ranking_results <- ranking_results[ranking_results$method!="poisson",]


# plot oos performance over seasons
out <- ggplot(ranking_results)
grid.arrange(out +
               geom_col(aes(season, kendalls_tau, fill = method), position = "dodge") +
               theme(legend.position="bottom"),
             out +
               geom_col(aes(season, spearmans_rho, fill = method), position = "dodge") +
               theme(legend.position="bottom"),
             ncol=2)
```



# Conclusion

In this paper, we evaluate two methods for simulating the outcome of the aborted local men's football league season 2019/2020 Recklinghausen Class A1. We provide a brief overview of the state of the literature on forecasting football game result. 90 out of 240 games were predicted with a weighted randomizing of the existing table as a benchmark, with a simulation based on the Elo ranking and with a Poisson/negative Binomial regression. We calculate an out-of-sample error for the preceding three seasons in form of rank-correlation coefficients of the predicted table rankings. We find that no method is clearly inferior or superior.

The decision to quit all games later than 8th of March because of the pandemic was not revised while the infection rates relaxed during May and June in Germany. Combined with the unforeseeable future of the SARS-Cov2 pandemic, a more fair and balanced decision making process could include statistical learning techniques, such as those shown in this paper.

We find that neither a simulation using a weighted coin toss based on the current league table (the points method), one based on the Elo ranking, or one based on regressions using (Quasi-)Poisson or negative binomial methods is superior. Therefore we cannot recommend one in particular. Testing prediction methods with data from local football league's may help improving and evaluating more existing methods from the literature.

\newpage

\printbibliography

\newpage


# Appendix

```{r , echo = FALSE, results = "asis", message = FALSE}
d <- readr::read_rds(stringr::str_c(here::here() , "data", "database_match_results_1920.rds", sep = "/")) 
d <- bind_rows(
  tibble(
    goals = d$goals_team_home,
    team = d$club_name_home,
    opponent=d$club_name_away,
    home= "y"),
  tibble(
    goals=d$goals_team_away,
    team=d$club_name_away,
    opponent=d$club_name_home,
    home= "n")
)

# create a fake model
# note that team needs to include all of your factors
fake <- lm(goals ~ home + team , d)
# rename the coefficients
names(fake$coefficients) <- gsub("team","",names(fake$coefficients))

# goodness-of-fit test function
gfit <- function(model){
  with(model, cbind(res.deviance = deviance, df = df.residual,
  p = pchisq(deviance, df.residual, lower.tail=FALSE)))
}

# 
m <- glm(goals ~ home + team + opponent, family = poisson(link=log), data = d)
m.s <- summary(m)
#gfit(m)

# Test for overdispersion
#library(AER)
#dispersiontest(m)

# overdispersion robust
m <- glm(goals ~ home + team + opponent, family = quasipoisson(link=log), data = d)
m.s <- summary(m)

# alternative: negative binomial distribution
library(MASS)
nb = glm.nb(goals ~ home + team + opponent, link = "log", data = d)
nb.s <- summary(nb)
#gfit(nb)

## write a function that fixes the names in the glm output
f <- function(x){
  names(x) <- gsub("team|opponent","", names(x))
  return(x)
}

stargazer(fake,fake,fake, header = FALSE,
          # coefficients
          coef = list(
            f( m.s$coefficients[grepl("Intercept|home", rownames(m.s$coefficients)), 1]),
            f( m.s$coefficients[grepl("team", rownames(m.s$coefficients)), 1]),
            f( m.s$coefficients[grepl("opponent", rownames(m.s$coefficients)), 1])
          ),
          # standard errors
          se = list(
            f( m.s$coefficients[grepl("Intercept|home", rownames(m.s$coefficients)), 2]),
            f( m.s$coefficients[grepl("team", rownames(m.s$coefficients)), 2]),
            f( m.s$coefficients[grepl("opponent", rownames(m.s$coefficients)), 2])
          ),
          title  = "19/20 Season Regression Output for the Quasi-Poisson Model"  ,
          column.labels = c("control","team", "opponent"),
          colnames = FALSE,
          # calculate pvalue using supplied coeff and se
          t.auto = T,
          #dep.var.caption  = "Dependend variable = goals",
          #out = "stargazer_data.html",
          omit.stat=c("all"),
          type = "latex",
          single.row = TRUE,
          order = c("Constant", "home"),
          align = TRUE,
          column.sep.width = "-30pt",
          font.size = "small",
          style = "aer"
          )
#Template table layout?
```



\input{append_rank_corr.tex}


